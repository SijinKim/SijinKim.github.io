---
title: "GAN theory(2) - global optimum"
excerpt: "GAN의 minmax problem으로 global optimum point가 보장되는 이유"
categories: 
   - Deep Learning
   - Machine Learning
   - GAN
   - KL Divergence
use_math: true
---

지난 포스트를 통해 GAN의 minmax problem을 통한 학습법의 global optima 수렴 보장성과 그 수렴값을 알아보았습니다.

이번 포스트는 *Kullback-Leibler Divergence(KL Divergence)*, *Jensen–Shannon divergence(JSD)*를 통해, "정말 그 수렴값이 global optimal value가 맞니?"에 대한 질문에 답이 될 것 입니다.

#### 1. 간단한 관련 이론 정리

- **Information**: 주어진 이벤트에서 놀라움의 양을 말합니다. 즉, 아주 적은 확률로 발생하는 이벤트가 발생할 경우 information의 값은 커지게 되죠.(발생할 확률이 극악으로 낮은 사건이 발생한다면, 사건의 결과를 떠나 다들 굉장히 놀라겠죠?)

    > $ I(E) = -log[Pr(E)] =-logP(X) $

    확률 이벤트 E에 negative log 취한 꼴. $ Pr(E) == 1$ 인 이벤트(100% 발생하는 이벤트)인 경우 information 값은 0. 확률 작을 수록 놀라움의 양은 극대! 즉, information의 양이 증가하는 것입니다. (x 가 0에 가까운 negative log function 생각해보면 알 수 있죠) 

- **Entropy**: 특정 이산 확률을 가지는 이벤트 E에 대한 information 값의 평균입니다. 즉, information 값에 기댓값을 취해 얻게되는 값을 말합니다.

    >$ Entropy\ H(X) $  
    >
    >$ = E[I(E)] = E[-logP(X)] = \sum_{i=1}^{n}{P(x_i)(-logP(x_i))}=  -\sum_{i=1}^{n}{P(x_i)logP(x_i)} $

- **KL Divergence(Relative Entropy)**: 두 확률 분포의 유사도를 측정하는 수단이라고 말하지만, $ D_{KL}(p\|\|q)\neq D_{KL}(q\|\|p) $ 이기 때문에 두 확률 분포의 유사도라는 것보다는 '근사한 확률 분포가 타겟 분포와의 유사도를 나타내주는 지표'라고 설명하는 것이 낫다고 생각합니다. 

    bayesian 관점에서 볼 때, prior distribution q에서 posterior distribution p로 이동할 때($D_{KL}(p\|\|q)$) 얻어지는 information 값을 의미합니다. p와 q의 분포가 비슷하다면, q를 사전정보로 하여 p의 분포를 얻는 것은 놀라운 일이 아니죠! 

    즉, p와 q의 분포가 비슷할수록 놀라움의 양의 줄고, KL Divergence 값 또한 작아지는 양상을 보입니다.

    그러나 개인적으로 저는 KL Divergence를 두 확률 분포 p, q 의 *log likelihood ratio(LR)*의 기댓값으로 이해하는 것이 가장 와닿는 것 같습니다. 
    
    Likelihood ratio 관점에서 살펴보겠습니다. 일단 Likelihood ratio를 통해 어떠한 값 x가 임의의 분포로부터 sampling할 때, q 분포보다 p 분포에서 뽑힐 가능성을 측정하게 됩니다.

    > $LR = \frac{p(x)}{q(x)} $, 
    >
    >여기에 독립적인 sample x가 n개 있다면 LR는 아래와 같이 구할 수 있습니다.
    >
    > $LR = \prod_{i=0}^{n}{\frac{p(x_i)}{q(x_i)}} $ 
    >
    > 이에 log를 취해 log LR를 구해봅시다.
    >
    > $logLR = log(\prod_{i=0}^{n}{\frac{p(x_i)}{q(x_i)}})= \sum_{i=0}^{n}{log(\frac{p(x_i)}{q(x_i)})}$

    위의 log likelihood ratio에 기댓값을 취해 n개의 sample x가 q 분포보다 p 분포에서 뽑힐 가능성을 정량화한 것이 바로 KL Divergence입니다.( Likelihood ratio에 타겟 분포함수로 하여 기댓값을 취함)

    $ E[logLR] $

    $ = \sum_{i=0}^{n}{p(x_i)log(\frac{p(x_i)}{q(x_i)})} $

    $ = \sum_{i=0}^{n}{p(x_i)log\ p(x_i)} - \sum_{i=0}^{n}{p(x_i)log\ q(x_i)} $

    $ = D_{KL}(p\|\|q) $


- **Cross-Entropy**

    $ Cross\ Entropy$ 

    $ = H(p) + D_{KL}(p\|\|q) $

    $ = -\sum_{i=0}^{n}{p(x_i)log\ p(x_i)} + \sum_{i=0}^{n}{p(x_i)log\ p(x_i)} - \sum_{i=0}^{n}{p(x_i)log\ q(x_i)} $

    $ = - \sum_{i=0}^{n}{p(x_i)log\ q(x_i)} $


- **Jensen–Shannon divergence(JSD)**: KL Divergence처럼 근사된 확률 분포가 타겟 분포와 유사한지를 측정하는 척도입니다. 다만, KL Divergence가 unsymmetric한 것과 달리 JSD는 symmetric한 특성을 갖습니다.($JSD(p\|\|q) = JSD(q\|\|p)$)

    > $ JSD(p\|\|q) = \frac{1}{2}D_{KL}(p\|\|m) + \frac{1}{2}D_{KL}(q\|\|m) $
    >
    > $, where\ m = \frac{1}{2}(p + q) $

    JSD 값의 범위는 $[0, \infty)$ 이고, p 와 q가 완전히 동일할 때 0의 값을 갖습니다.


#### 2. global minimum value의 보장(추가 증명)

이전 포스트에서 generator fix한 가정 하에 optimal discriminator가 구해지는 것과, generator 또한 타겟 데이터의 분포에 최적화됐다는 것이 가정됐을 때 -log4 의 optimal value가 획득된다는 것을 알 수 있었습니다.

$ For\ p_{data} = p_{gen}, \ $
$ \underset{D}{max}\ V(G,D) $

$ = E_{x\sim p_{data}}[log \frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)}] + E_{x\sim p_{gen}}[log(1 - \frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)})] $

$ = E_{x\sim p_{data}}[log \frac{1}{2}] + E_{x\sim p_{gen}}[log(1 - \frac{1}{2})]$

$ = -log2 -log2 = -log4 $

---

더하고 빼기 트릭을 사용해 위 식$(C(G))$을 reformulate 합시다.

$ C(G) = C(G) + (log4 -log4) $ 

$ = -log4 + (C(G) + log4) $

$ = -log4 + (E_{x\sim p_{data}}[log \frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)}] + E_{x\sim p_{gen}}[log(1 - \frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)})] + log2 + log2)$

$ = -log4 + (E_{x\sim p_{data}}[log \frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)}] + E_{x\sim p_{gen}}[log \frac{p_{gen}(x)}{p_{data}(x) + p_{gen}(x)}] + log2 + log2)$

$ = -log4 + E_{x\sim p_{data}}[log 2 *\frac{p_{data}(x)}{p_{data}(x) + p_{gen}(x)}] + E_{x\sim p_{gen}}[log 2 *\frac{p_{gen}(x)}{p_{data}(x) + p_{gen}(x)}]$

$ = -log4 + E_{x\sim p_{data}}[log\ p_{data}(x) *\frac{2}{p_{data}(x) + p_{gen}(x)}] + E_{x\sim p_{gen}}[log\ p_{gen}(x) *\frac{2}{p_{data}(x) + p_{gen}(x)}]$

KL Divergence 형식이 보이시나요? 더해준 log4 를 log2+log2로 나눠 각 expextation에 계산해줌으로써 KL Divergence formula 형식으로 정리할 수 있었습니다. 그 결과 $C(G)$는 아래와 같이 정리됩니다.

$ = -log4 + D_{KL}(p_{data}(x)\|\| \frac{p_{data}(x) + p_{gen}(x)}{2}) + D_{KL}(p_{gen}(x)\|\| \frac{p_{data}(x) + p_{gen}(x)}{2})$


위 식은 다시한번 JSD 형식에 맞춰 정리됨을 알 수 있습니다. 따라서 최종식은 아래와 같이 정리됩니다.

$ C(G) = -log4 + 2 * JSD(p_{data}\|\|p_{gen})$

앞서 언급했듯이 JSD는 항상 양수이고, P와 Q가 동일할 때만 0의 값을 갖습니다.

즉, $p_{data}$와 $p_{data}$가 동일할 때(generator의 학습이 완벽하게 이뤄진 경우: 우리가 바라는 global optima 상황) JSD 항은 0이 되고 최적값으로 -log4가 도출됨을 확인할 수 있습니다!

